{
  "agent_id": "coder1",
  "task_id": "task_0",
  "files": [
    {
      "name": "main_model.py",
      "purpose": "Main computer vision model",
      "priority": "high"
    },
    {
      "name": "training.py",
      "purpose": "Training pipeline",
      "priority": "high"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.LG_2507.23620v1_DivControl_Knowledge_Diversion_for_Controllable_I",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.LG_2507.23620v1_DivControl-Knowledge-Diversion-for-Controllable-I with content analysis. Detected project type: computer vision (confidence score: 7 matches).",
    "key_algorithms": [
      "Your",
      "Universal",
      "Transferable",
      "Guided",
      "Decomposes",
      "Prediction",
      "Hed",
      "Language",
      "Denoising",
      "Age"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.LG_2507.23620v1_DivControl-Knowledge-Diversion-for-Controllable-I.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nDivControl: Knowledge Diversion for Controllable Image Generation\nYucheng Xie1,2Fu Feng1,2Ruixiao Shi1,2Jing Wang1,2*Yong Rui3Xin Geng1,2*\n1School of Computer Science and Engineering, Southeast University, Nanjing, China\n2Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary\nApplications (Southeast University), Ministry of Education, China\n3Lenovo Research\n{xieyc, fufeng, wangjing91, xgeng }@seu.edu.cn\nAbstract\nDiffusion models have advanced from text-to-image (T2I)\nto image-to-image (I2I) generation by incorporating struc-\ntured inputs such as depth maps, enabling fine-grained spa-\ntial control. However, existing methods either train sep-\narate models for each condition or rely on unified archi-\ntectures with entangled representations, resulting in poor\ngeneralization and high adaptation costs for novel condi-\ntions. To this end, we propose DivControl , a decompos-\nable pretraining framework for unified controllable genera-\ntion and efficient adaptation. DivControl factorizes Con-\ntrolNet via SVD into basic components\u2014pairs of singu-\nlar vectors\u2014which are disentangled into condition-agnostic\nlearngenes and condition-specific tailors through knowl-\nedge diversion during multi-condition training. Knowledge\ndiversion is implemented via a dynamic gate that performs\nsoft routing over tailors based on the semantics of con-\ndition instructions, enabling zero-shot generalization and\nparameter-efficient adaptation to novel conditions. To fur-\nther improve condition fidelity and training efficiency, we\nintroduce a representation alignment loss that aligns con-\ndition embeddings with early diffusion features. Extensive\nexperiments demonstrate that DivControl achieves state-of-\nthe-art controllability with 36.4 \u00d7less training cost, while\nsimultaneously improving average performance on basic\nconditions. It also delivers strong zero-shot and few-shot\nperformance on unseen conditions, demonstrating superior\nscalability, modularity, and transferability.\n1. Introduction\nDiffusion models have demonstrated remarkable perfor-\nmance in text-to-image (T2I) generation, with models such\nas DALL-E 3 [19], Stable Diffusion 3 [3], and Midjour-\nney [12] generating images comparable to human-created\n*Corresponding authors\nEfficientFinetuneTraditionalControlNetTime-Consuming(a)(b)PoorTransferabilityFailKnowledgeDiversion\u2026Tailor2\nUniversal Control\nControlNetNovelCondition\n(c)(Similar)(LargeShift)Learn-geneNovelConditionZero-shotGenerationLearn-geneLearn-gene\n(d)CN1CN2CNTTailor1TailorN\u2026\u2026\u2026\nAverageCLIPScoreTailor1TailorN\nRandomRandomDivControlCtrLoRAUniControl1,0002,0003,0004,0005,0006,00090.5091.0091.5092.00GPUHours36.4\u00d7Figure 1. (a) Traditional ControlNet requires training a dedicated\nmodel for each control type, leading to substantial computational\noverhead. (b) Universal control approaches aim to unify all con-\nditions within a single model, but exhibit poor generalization to\nunseen tasks. (c) DivControl addresses this by introducing knowl-\nedge diversion to disentangle condition-agnostic and condition-\nspecific representations during training, enabling unified control\nand zero-shot transfer. (d) This modular design reduces training\ncost by over 36.4\u00d7(165 vs. 6000 GPU hours) while achieving\nsuperior controllable generation quality.\nartwork from natural language prompts [1, 17, 34]. Their\nability to produce high-fidelity, semantically aligned out-\nputs has positioned diffusion models as a foundation for\ncontrollable generation. To support finer-grained and more\ndeterministic control, recent efforts have extended con-\nditions beyond text to structured visual inputs\u2014such as\ndepth and segmentation maps\u2014enabling image-to-image\n(I2I) generation with explicit spatial guidance [13, 33, 34].\nHowever, training separate diffusion models for each\ncontrol condition is computationally prohibitive. For in-arXiv:2507.23620v1  [cs.CV]  31 Jul 2025\n\n--- Page 2 ---\nstance, ControlNet [34] requires over 600 A100 GPU hours\non 3 million images to support a single C ANNY condition,\nwith other modalities demanding even more resources [31].\nTo improve efficiency and scalability, recent works propose\nunified frameworks for multi-condition control [18, 23, 25],\naiming to handle diverse conditions within a single model\n(Figure 1a, b). However, these approaches struggle to gen-\neralize to unseen or heterogeneous conditions, as jointly\nlearned representations tend to be entangled, hindering\nadaptation to novel control conditions.\nWhile CtrLoRA [31] improves transferability by assign-\ning a dedicated LoRA [7] to each condition during train-\ning, thereby shaping a more adaptable ControlNet, it re-\nmains computationally intensive due to large-scale multi-\ncondition pretraining. More critically, its rigid separation\nmechanism fails to account for intrinsic inter-condition cor-\nrelations, leading to suboptimal performance, limiting mod-\nular reuse and hindering generalization to unseen condi-\ntions.\nRecently, knowledge diversion [30] was introduced\nto explicitly disentangle task-agnostic and task-specific\nknowledge by applying Singular Value Decomposition\n(SVD) to factorize network weight matrices into shared\nlearngenes and task-specific tailors, thereby improving\nmodular reuse and cross-task transferability. Building on\nthis principle, we propose DivControl, which brings knowl-\nedge diversion into controllable image generation. By fac-\ntorizing ControlNet into shared learngenes and lightweight\ntailors during training, DivControl supports unified genera-\ntion across diverse conditions and enables efficient adapta-\ntion to novel conditions with minimal overhead.\nDivControl applies SVD to decompose each weight\nmatrix into basic components\u2014pairs of singular vec-\ntors\u2014which are modularly assigns them to shared learn-\ngenes or condition-specific tailors for structured control.\nTo improve parameter sharing and generalization, DivCon-\ntrol replaces the binary gate [30, 31] with a dynamic gate.\nSpecifically, each condition is described by a textual in-\nstruction, which is encoded into a condition text embedding\nusing a pretrained text encoder [16]. The embedding guides\nthe dynamic gate to assign soft weights over tailors, follow-\ning a mixture-of-experts style [20, 36] to facilitate modular\nreuse and enhance zero-shot generalization to novel con-\nditions. To enhance convergence and semantic alignment\nduring knowledge diversion, we incorporate a representa-\ntion alignment module [32], which aligns condition im-\nage embeddings with shallow diffusion features.This aux-\niliary supervision improves early feature learning, enhances\nknowledge decomposition, and strengthens alignment be-\ntween generated images and target conditions.\nDivControl is pretrained via knowledge diversion on\nSubject200K [23] using 8 base conditions and evaluated\non COCO2017 [11] with 10 additional unseen conditionsto assess generalization and transferability. Remarkably,\nDivControl requires only 450K training steps ( \u223c165 GPU\nhours), achieving a 36.4\u00d7 reduction in computational cost\ncompared to CtrLoRA (6000 GPU hours [31]), while im-\nproving unified controllability with average CLIP-I gains\nof 0.05, respectively, on base conditions. For unseen con-\nditions, DivControl demonstrates strong zero-shot general-\nization, generating high-quality outputs on low-shift condi-\ntions (e.g., G RAYSCALE and L INEART ) without finetuning.\nFor high-shift conditions, DivControl achieves state-of-the-\nart performance by finetuning only the tailors at minimal\ncost (\u223c0.23 GPU hours and 200 images), offering a sharp\ncontrast to ControlNet, which requires over 600 GPU hours\nper condition for retraining.\nOur main contributions are as follows: 1) We introduce\nDivControl, the first decomposable framework for control-\nlable image generation. By factorizing ControlNet through\nknowledge diversion, DivControl enables modular, inter-\npretable, and transferable generation with substantially re-\nduced computational overhead. 2) We propose a representa-\ntion alignment mechanism that bridges condition inputs and\ndiffusion features, enhancing controllability and accelerat-\ning convergence in controllable image generation. 3) We\nconstruct a benchmark with 18 diverse control conditions\nto evaluate unified controllable generation, as well as trans-\nferability and generalization of trained models. Extensive\nexperiments demonstrate that DivControl consistently out-\nperforms prior methods across both seen and unseen condi-\ntions.\n2. Related Work\n2.1. Controllable Image Generation\nDiffusion models have achieved significant progress in text-\nto-image (T2I) synthesis, producing high-fidelity images\nacross diverse prompts [1, 14, 17]. Recent efforts extend\ncontrol to spatial conditions, such as depth and segmenta-\ntion maps [2, 13]. ControlNet [34] exemplifies this by in-\ntroducing separate branches, but training separate models\nfor each condition is computationally expensive. Adapter-\nbased alternatives [13, 33] reduce cost but remain data-\nintensive.\nTo enhance flexibility, recent methods explore unified\ncontrollable generation, aiming to use a single model across\ndiverse conditions [8, 18, 35]. However, these models strug-\ngle to generalize to novel or semantically divergent con-\nditions. We address this by introducing a unified frame-\nwork that integrates modular parameter decomposition with\ndynamic routing. By disentangling condition-agnostic and\ncondition-specific knowledge during training, our method\nenables scalable generation across diverse conditions and\nefficient adaptation to unseen conditions, even under lim-\nited resources.\n\n--- Page 3 ---\n2.2. Learngene and Knowledge Diversion\nThe L EARNGENE framework, inspired by biological inher-\nitance [4, 28], encodes task-agnostic knowledge into mod-\nular neural units for efficient transfer [5]. Existing ap-\nproaches mainly emphasize knowledge compression and\nreuse\u2014either through heuristic layer selection [27, 28] or\nstructured operations such as Kronecker products [6]. How-\never, they largely focus on representation reuse without ex-\nplicit task-level disentanglement, limiting adaptability in\nmulti-task and cross-domain scenarios.\nTo address this, knowledge diversion [30] decomposes\nmodel parameters into task-agnostic learngenes and task-\nspecific tailors, facilitating modular reuse through gated\nrouting. Recent works, such as FAD [22], apply this to\nthe frequency domain for few-shot adaptation. We extend\nthis framework to controllable image generation by disen-\ntangling ControlNet parameters across conditions, enabling\nunified multi-condition generation and efficient adaptation\nto novel conditions.\n3. Methods\n3.1. Preliminary\n3.1.1. Latent Diffusion Models\nLatent diffusion models (LDMs) shift the generative\nprocess from high-dimensional pixel space to a lower-\ndimensional latent space. To enable this, an autoencoder E\nencodes an image xinto a latent representation z=E(x),\nand a diffusion model is trained to reconstruct zthrough a\ndenoising process, minimizing:\nLdiff=Ez,c,\u03b5,th\n||\u03b5\u2212\u03b5\u03b8(zt|c, t)||2\n2i\n, (1)\nwhere \u03b5\u03b8is a noise prediction network that predicts the\nnoise \u03b5added to ztat timestep t, conditioned on c.\n3.1.2. Conditional Generation\nIn conditional LDMs, the condition cis derived from ex-\nternal modalities such as text, class labels, or condition im-\nages. We focus on image-to-image (I2I) generation, where a\ncondition image xcond(e.g., a depth map) provides structural\nguidance. A condition encoder F(e.g., ControlNet [34])\nmaps xcondto an embedding c=F(xcond), which is in-\njected into the denoising network via adding or similar fu-\nsion strategies.\nUnlike prior methods that adopt fixed [18, 31] or task-\nspecific [13, 33] encoders, we apply knowledge diver-\nsion to modularize F. Specifically, we decompose it\ninto condition-agnostic components (i.e., learngenes) and\ncondition-specific components (i.e., tailors), enabling scal-\nable and efficient adaptation to diverse conditions.3.2. Knowledge Diversion in ControlNet\n3.2.1. Decomposition of ControlNet\nWe adopt a Diffusion Transformer (DiT) [2, 3, 17] as the\nbackbone for generation, with the corresponding Control-\nNetFsharing the same transformer-based architecture, as\nshown in Figure 2. To enable flexible adaptation to di-\nverse conditions, we apply knowledge diversion within F, a\ntransformer-based module composed of Llayers, each con-\ntaining repeated projection matrices in attention and MLP\nblocks. The parameter set is denoted as \u03b8={W(1\u223cL)\nq ,\nW(1\u223cL)\nk, W(1\u223cL)\nv , W(1\u223cL)\no , W(1\u223cL)\nin , W(1\u223cL)\nout}1.\nFollowing the decomposition strategy of KIND [30],\neach weight matrix W(l)\n\u22c6(\u22c6\u2208 S={q, k, v, o, in, out }and\nl\u2208[1, L]) is factorized via SVD as:\nW(l)\n\u22c6=U(l)\n\u22c6\u03a3(l)\n\u22c6V(l)\n\u22c6\u22a4=rX\ni=1u(l,i)\n\u22c6\u03c3(l,i)\n\u22c6v(l,i)\n\u22c6 (2)\nwhere \u0398(l,i)\n\u22c6= (u(l,i)\n\u22c6, \u03c3(l,i)\n\u22c6, v(l,i)\n\u22c6)denotes the i-th rank-1\ncomponent, and ris the rank of W(l)\n\u22c6. Each component\n\u0398(l,i)\n\u22c6captures a modular unit of structured knowledge.\n3.2.2. Dynamic Diversion via Condition Text Embedding\nTo support flexible adaptation across heterogeneous condi-\ntions, these components are explicitly partitioned into NG\ncondition-agnostic learngenes GandNTcondition-specific\ntailors T, where r=NG+NT. Formally:\nG=n\n\u0398(l,i)\n\u22c6|i\u2208[0, NG), \u22c6\u2208 S, l\u2208[1, L]o\nT=n\n\u0398(l,i)\n\u22c6|i\u2208[NG, NG+NT), \u22c6\u2208 S, l\u2208[1, L]o\nUnlike KIND [30], which relies on discrete class labels\nfor component assignment, or CtrLoRA [31], which em-\nploys fixed task-specific adapters, we introduce a continu-\nous diversion mechanism to enable generalization to hetero-\ngeneous and unseen conditions. To compensate for the lim-\nited semantics in condition images, each condition is paired\nwith a manually defined condition instruction tcond, which\nis encoded into the semantic embedding etxt=Etxt(tcond)\nusing a pretrained text encoder.\nTo enable dynamic adaptation, the condition embed-\ndingetxtis processed by a lightweight gating module G,\nanalogous to the router in Mixture-of-Experts architec-\ntures [20, 36]. This dynamic gate produces soft weights\noverNTtailor components:\n\u03b1=softmax (G(etxt))\u2208RNT(3)\nwhere \u03b1denotes globally shared mixing coefficients, ap-\nplied uniformly across all layers l\u2208[1, L]and projection\n1W(1\u223cL)\nq denotes the set {W(1)\nq,W(2)\nq,...,W(L)\nq}. Similar nota-\ntions throughout the paper follow this convention.\n\n--- Page 4 ---\n\u00d7\u00d7!!!\"\u2026\u2026!\"\u2026\u2026!\"\u2026\u2026!\"Multi-HeadSelf-AttentionPointwiseFeedforwardConditionInstruction\u201csegmentation map to image\u201dDynamicGateVisualEncoder\n(a)Knowledge DiversionOutput\u2026TextEncoder\n(b)RepresentationAlignmentLREPASegmentationSketchNormalHedControlNetDiffusionTransformer\nConditionImageDiTBlock1DiTBlockiDiTBlocki+1DiTBlockN\u2026\u2026CNBlock1CNBlocki\u2026\u202600.20.500.3Depth\nMLPVisualRepresentationDiffusionRepresentation\nFigure 2. Overview of the DivControl. (a) Each weight matrix in ControlNet is factorized via SVD into condition-agnostic learngenes and\ncondition-specific tailors. A dynamic gate routes each input to relevant tailors while jointly updating shared learngenes, enabling modular\nand disentangled representation across conditions. (b) Shallow features in ControlNet are aligned with condition semantics extracted by a\npre-trained vision encoder, improving consistency and accelerating convergence.\n\u00d7\u00d7!NovelCondition:DirectSelectTailors(Zero-shot)RandomLearngene\nFrozenTrainable(a)Low-shift Novel Conditions\nGRAYSACLEDynamicGate\u202600.10.600.3\u2026!\"\n\u2026!\n\"\u2026!\n\"\n!\u00d7\u00d7\u2026\n!\"\n\u2026\n!\"#\u2026\n!\"(b)High-shift Novel Conditions\nNovelCondition:PALETTEAddRandomTailors(EfficientFinetuning)\nFigure 3. Generalization to Novel Control Conditions. (a) For\nlow-shift conditions, DivControl leverages instruction embed-\ndings to dynamically activate semantically aligned tailors, en-\nabling zero-shot generation. (b) For high-shift conditions, it reuses\ncondition-agnostic learngenes while introducing randomly initial-\nized tailors, supporting efficient few-shot adaptation.\ntypes \u22c6\u2208 S. These coefficients modulate tailor compo-\nnents through weighted aggregation, enabling flexible and\ncondition-aware adaptation.\nDuring training, the condition embedding econdactivates\nrelevant tailor components via a dynamic gate, while sharedlearngenes are updated across all conditions. The condition-\nadaptive weight matrix is constructed as a gated combina-\ntion of learngenes and tailors:\nfW(l)\n\u22c6=G(l)\n\u22c6+KX\nk=1\u03b1\u00b7 T(l)\nk,\u22c6. (4)\nThis facilitates explicit disentanglement of condition-\nagnostic and condition-specific knowledge, with all compo-\nnents\u2014learngenes, tailors, and the dynamic gate\u2014jointly\noptimized via end-to-end training (see Algorithm 1 in Ap-\npendix).\n3.3. Representation Alignment\nRepresentation Alignment (REPA) [32] was originally pro-\nposed to improve training efficiency and synthesis qual-\nity in class-conditioned diffusion models by aligning in-\ntermediate features with those from pretrained vision en-\ncoders [9, 10, 24, 29]. We extend REPA to controllable\nimage generation, where it facilitates faster optimization of\nthe condition encoder and improves control fidelity.\nGiven a condition image xcond, we extract a semantic em-\nbedding eimg=Eimg(xcond)\u2208RN\u00d7dusing a frozen vision\nencoder. Simultaneously, a shallow feature fcond\u2208RN\u00d7d\u2032\nis obtained from early layers of the ControlNet F. A\nlightweight MLP head A(\u00b7)is trained to align fcondwith\neimgvia\nLREPA=\u2212Ez,c,\u03b5,t\"\n1\nNNX\nn=1sim(A(fcond)[n], e[n]\nimg)#\n(5)\n\n--- Page 5 ---\nwhere nis the patch index. This alignment encourages F\nto learn semantically grounded features, improving conver-\ngence while enhancing the reliability of condition-guided\ngeneration. The final objective combines denoising and\nalignment losses to optimize both generative fidelity and\ncontrol semantics:\nLtotal=Ldiff+\u03bb\u00b7 LREPA, (6)\nwhere Ldiffis the standard denoising loss (Eq. (1)), and \u03bb\nbalances the contribution of the alignment regularization.\n3.4. Efficient Adaptation to Novel Conditions\nThrough knowledge diversion, DivControl factorizes Con-\ntrolNet into condition-agnostic learngenes and condition-\nspecific tailors, enabling rapid adaptation to novel condi-\ntions via reusable general features and modular activation\nof specialized components.\nGiven a low-shift novel condition xlow, we directly en-\ncode corresponding condition instruction via a pretrained\ntext encoder to obtain elow=Etxt(tlow), which is fed into\nthe dynamic gate Gto compute soft routing weights over\nNTtailor components, as shown in Figure 3a.\n\u03b1low=softmax (G(elow)). (7)\nLeveraging the semantic generalization of Etxt, this mecha-\nnism enables direct zero-shot generation for unseen condi-\ntions without gradient updates or task-specific retraining.\nFor conditions xhighwith substantial semantic shifts\n(Figure 3b), adaptation remains lightweight by introducing\nrandomly initialized tailors while keeping transferred pa-\nrameters frozen, enabling localized fine-tuning without dis-\nrupting previously acquired general knowledge.\nThis modular adaptation strategy promotes generaliza-\ntion, facilitates cross-condition transfer, and ensures effi-\ncient adaptation with minimal overhead.\n4. Experimental Setup\n4.1. Dataset\nWe perform knowledge diversion on Subject200K [23], a\nlarge-scale dataset containing 200K FLUX-generated [21]\nimages prompted by GPT-4o [15]. We annotate each im-\nage with 8 basic conditions for unified conditional genera-\ntion. For evaluation, we follow CtrLoRA [31] and use the\nCOCO2017 [11] validation set, extended with 10 additional\nconditions spanning both low- and high-shift distributions\nto assess DivControl\u2019s zero-shot generalization and efficient\nfinetuning capabilities. Annotation details and examples are\nprovided in the Appendix B.1.\n4.2. Basic Setting\nWe build on PixArt- \u03b4[2], adopting a DiT backbone with\na64\u00d764latent resolution. The ControlNet Fsharesthe same 13-layer transformer architecture. Models are\ntrained for 450K steps using AdamW with a learning rate\nof1.25\u00d710\u22125, weight decay of 3\u00d710\u22122, and batch size\n16. A MultiStepLR scheduler decays the learning rate by\n0.4 at 300K steps.\nFor knowledge diversion, we set the number of learn-\ngenes NGand tailors NTto 576, with 288 active tailor\ncomponents per condition. For REPA, features from the 4-\nth layer of Fare aligned with DINOv2-B [16] embeddings\nvia an alignment loss weighted by \u03bb= 0.05. Additional\nhyperparameter details are provided in Appendix B.2.\n5. Results\n5.1. Universal Generation on Basic Conditions\nDivControl enables unified controllable generation by\ndecomposing ControlNet into reusable learngenes and\ncondition-specific tailors through knowledge diversion,\nsupporting scalable multi-condition control. As shown in\nTable 1, DivControl outperforms CtrLoRA [31] on all met-\nrics in the average of eight basic conditions (Section 4.1),\nwhile reducing training time to just 165 GPU hours\u2014much\nlower than the 6000 and 5000 hours required by Ctr-\nLoRA and UniControl, respectively\u2014demonstrating both\nefficiency and strong generalization.\nIn contrast, existing methods face structural bottlenecks.\nUniControl [18] employs a shared encoder that lacks task-\nspecific specialization. CtrLoRA introduces modularity\nthrough LoRA but relies on static binary gate without\nsemantic-aware routing or representation disentanglement,\nlimiting transferability and reuse.\nDivControl mitigates these limitations through a dy-\nnamic gate, which enables semantic-aware selection of\ncondition-specific tailors based on condition embeddings.\nBy explicitly disentangling condition-agnostic learngenes\nfrom condition-specific modulations, it promotes modu-\nlar reuse, accelerates convergence, and enhances scalabil-\nity\u2014highlighting the value of structured decomposition for\nunified controllable generation.\n5.2. Generalization to Novel Control Conditions\nWe evaluate DivControl\u2019s generalization to unseen condi-\ntions, categorized as: (1) low-shift conditions that are se-\nmantically close to training modalities, and (2) high-shift\nconditions with substantial domain or modality gaps.\nLeveraging knowledge diversion and dynamic gate, Div-\nControl enables zero-shot generation in low-shift settings\nand lightweight adaptation in high-shift scenarios. We\npresent analyses for both cases below.\n5.2.1. Zero-shot Generalization on Low-shift Conditions\nFor novel conditions with minor semantic or structural de-\nviations from training conditions, DivControl enables zero-\nshot generalization by embedding condition instructions\n\n--- Page 6 ---\nTable 1. Performance on Basic Control Conditions. We report LPIPS ( \u2193), SSIM ( \u2191), and CLIP-I ( \u2191) across eight basic control conditions\nto evaluate generation fidelity and semantic alignment. \u201cGPU Hour\u201d indicates total training time, and \u201cPara.\u201d denotes the average number\nof trainable parameters, reflecting overall training efficiency.\nBBOX CANNY DEPTH HED Cost\nLPIPS SSIM CLIP-I LPIPS SSIM CLIP-I LPIPS SSIM CLIP-I LPIPS SSIM CLIP-I GPU Hour Para.\nPixart- \u03b4 0.255 0.766 89.43 0.283 0.473 93.69 0.232 0.823 93.20 0.246\u22170.675\u221795.00\u22178\u00d736 h 8\u00d7295M\nUniControl 0.229\u22170.777 90.11\u22170.249 0.500 94.97 0.229 0.837\u221793.68 0.228 0.704 95.44 5000 h 374M\nCtrLoRA-SD 0.221 0.788 90.50 0.316 0.404 93.66 0.218 0.841 94.25 0.285 0.609 94.27 6000 h 656M\nCtrLoRA-PA 0.249 0.768 89.62 0.284 0.473\u221793.59 0.236 0.812 93.36 0.270 0.638 94.30 165 h 519M\nDivControl 0.237 0.779\u221789.99 0.274\u22170.466 94.03\u22170.223\u22170.833 93.79\u22170.259 0.657 94.55 165 h 477M\u2217\nSKETCH NORMAL OUTPAINTING SEGMENTATION Average\nLPIPS SSIM CLIP-I LPIPS SSIM CLIP-I LPIPS SSIM CLIP-I LPIPS SSIM CLIP-I LPIPS SSIM CLIP-I\nPixart- \u03b4 0.260 0.718 91.70 0.400 0.705 89.96 0.061 0.898 93.42 0.458 0.652 87.92 0.274 0.714 91.79\nUniControl 0.344 0.628 87.22 0.382\u22170.728 90.58\u22170.066 0.909 93.17 0.461 0.647 87.61\u22170.273 0.716\u221791.72\nCtrLoRA-SD 0.256\u22170.721 92.08\u22170.377 0.710\u221791.15 0.072 0.893 92.95 0.437 0.662 89.03 0.273\u22170.703 92.24\u2217\nCtrLoRA-PA 0.257 0.721\u221791.67 0.400 0.697 90.16 0.058\u22170.914\u221793.80\u22170.447 0.656\u221788.35 0.275 0.710 91.85\nDivControl 0.242 0.741 92.25 0.386 0.710 90.53 0.053 0.920 94.51 0.445\u22170.656 88.66 0.265 0.720 92.29\nConditionOriginalUniControlCtrLoRADivControlILLUSION\nLINEART\nBLUR\nINPAINTING\nBRUSH\nGRAYSCALEConditionOriginalUniControlCtrLoRADivControl\nFigure 4. Zero-shot Generalization on Low-shift Novel Conditions. We visualize qualitative results on unseen conditions that are\nsemantically aligned with training conditions. DivControl achieves effective zero-shot generation by leveraging knowledge diversion and\ndynamic gating, which route condition inputs to semantically relevant tailor components without any fine-tuning.\ninto task representations that guide the dynamic gate to\nsoftly activate semantically aligned tailors. As shown in\nFigure 4, DivControl consistently produces high-fidelity,\ncondition-aligned images across all low-shift novel condi-\ntions without requiring gradient updates.\nIn contrast, CtrLoRA [31] only assigns base ControlNet\nto each new condition, lacking a mechanism for semantic\ntransfer and thus failing at zero-shot adaptation. UniCon-\ntrol [18] manually composes base-condition combinations\nfor novel tasks, limiting flexibility and scalability. DivCon-\ntrol overcomes these limitations through input-conditioned\nrouting over dynamically selected tailors, enabling modu-\nlar reuse and scalable generalization to semantically related\nconditions.\n5.2.2. Efficient Finetuning on High-shift Conditions\nFor novel conditions with substantial distributional shifts,\nsuch as P ALETTE and S HUFFLE , we transfer the parametersand initialize tailors from scratch, as these cases exhibit sig-\nnificant visual discrepancies and demand distinct generative\npriors.\nAs shown in Table 2, despite substantial distribution\nshifts, DivControl achieves competitive performance after\njust 3K finetuning steps ( \u223c0.23 GPU hours) on 200 images,\nimproving average CLIP-I by 1.72 points over CtrLoRA.\nThis demonstrates its ability to rapidly adapt to novel condi-\ntions with minimal computational overhead. Furthermore,\nunlike full-model retraining which ignores transferability\nand incurs substantial cost in both training time and param-\neters per condition, DivControl enables rapid, lightweight\nadaptation while preserving fidelity and consistency.\nThese results highlight the scalability and flexibility\nof DivControl, enabled by its structural decoupling of\ncondition-agnostic and condition-specific knowledge. This\nmodular design supports adaptive parameter transfer for di-\n\n--- Page 7 ---\nBLUR BRUSH GRAYSCALE INPAINTING Cost\nLPIPS SSIM CLIP-I LPIPS SSIM CLIP-I LPIPS SSIM CLIP-I LPIPS SSIM CLIP-I GPU Time Para.\nPixart- \u03b4 0.179 0.923 94.70 0.168 0.769 94.56 0.209 0.708 96.92 0.149\u22170.771 96.45 1.01 295M\nPixart- \u03b4-canny 0.162\u22170.933\u221795.20\u22170.148\u22170.792\u221795.65\u22170.195\u22170.723\u221797.18\u22170.160 0.771\u221796.46 1.01 295M\nCtrLoRA-SD 0.204 0.875 93.79 0.167 0.729 95.15 0.262 0.599 95.49 0.197 0.697 95.34 0.93 37M\nCtrLoRA-PA 0.171 0.927 94.95 0.158 0.777 95.60 0.201 0.703 97.13 0.160 0.767 96.49\u22170.23 14M\nDivControl 0.143 0.946 95.84 0.136 0.800 96.28 0.195 0.724 97.33 0.144 0.778 96.97 0.23 14M\nJPEG PALETTE PIXEL SHUFFLE Average\nLPIPS SSIM CLIP-I LPIPS SSIM CLIP-I LPIPS SSIM CLIP-I LPIPS SSIM CLIP-I LPIPS SSIM CLIP-I\nPixart- \u03b4 0.308 0.630 92.18 0.254 0.675 88.54 0.390 0.576 87.42 0.685 0.215 84.80 0.293 0.658 91.95\nPixart- \u03b4-canny 0.300\u22170.656\u221792.92\u22170.234 0.724\u221788.89 0.366\u22170.616\u221789.34\u22170.679\u22170.229 84.48 0.281\u22170.681\u221792.51\u2217\nCtrLoRA-SD 0.356 0.570 92.07 0.285 0.663 87.48 0.465 0.500 86.37 0.692 0.196 84.74\u22170.328 0.603 91.30\nCtrLoRA-PA 0.328 0.635 92.67 0.228\u22170.721 89.24\u22170.383 0.588 88.76 0.681 0.237\u221783.84 0.289 0.669 92.34\nDivControl 0.297 0.668 94.23 0.224 0.736 89.30 0.365 0.640 89.84 0.674 0.244 84.74 0.272 0.692 93.07\nTable 2. Performance on Novel Control Conditions. We report LPIPS ( \u2193), SSIM ( \u2191), and CLIP-I ( \u2191) across 6 high-shift novel conditions\nand 2 low-shift novel conditions to evaluate generation fidelity and semantic alignment. \u201cGPU Hour\u201d indicates total training time, and\n\u201cPara.\u201d denotes the average number of trainable parameters.\nverse downstream demands, making it well suited for open-\nworld scenarios with evolving or unseen conditions.\n5.3. Multi-Conditional Controllable Generation\nDivControl enables unified controllable image generation\nby disentangling condition-specific knowledge into inde-\npendently activatable tailors, which further supports flexible\ncomposition of multiple control conditions through selec-\ntive feature reuse\u2014enabling more challenging, fine-grained\ncontrollable generation.\nBy aggregating task embeddings from multiple condi-\ntions, DivControl activates the corresponding tailor mod-\nules, enabling seamless integration of complex controls\nsuch as L INEART and P ALETTE . As shown in Figure 5,\nDivControl produces high-quality and semantically consis-\ntent results even under complex, combined control condi-\ntions. This demonstrates that modular routing effectively\npreserves the guidance from each input while allowing them\nto work together for coherent generation.\n5.4. Ablation and Analysis\n5.4.1. Ablation Experiments\nTo evaluate the impact of knowledge diversion and REPA,\nwe ablate each component individually. As shown in Ta-\nble 3, introducing knowledge diversion alone yields no-\ntable gains, with LPIPS decreasing by 0.026, SSIM increas-\ning by 0.03, and CLIP-I improving by 1.20%, highlighting\nthe effectiveness of modularizing condition-agnostic and\ncondition-specific knowledge for improved generalization.\nIncorporating REPA on top yields additional gains, with\nLPIPS further reduced by 0.012 and continued improve-\nments in SSIM and CLIP-I, confirming its effectiveness\nin enhancing semantic alignment. These results under-\nscore the complementary roles of both components: knowl-Table 3. Ablation study on DivControl.\nDiversion REPA LPIPS \u2193SSIM\u2191CLIP-I \u2191\n#1 0.328 0.663 89.43\n#2 \u2713 0.302 0.693 90.63\nDivControl \u2713 \u2713 0.290 0.696 91.31\nedge diversion enables flexible adaptation, while REPA\nreinforces semantic grounding\u2014together facilitating high-\nquality, controllable generation across diverse conditions.\n5.4.2. Analysis on Dynamic Gate\nTo analyze the routing behavior of the dynamic gate, we vi-\nsualize activation patterns across conditions. As shown in\nFigure 6, semantically related conditions (e.g., SKETCH and\nLINEART ) yield similar tailor activations, whereas dissimi-\nlar ones (e.g., SKETCH vs.OUTPAINTING ) exhibit divergent\nrouting.\nThese results indicate that the dynamic gate effectively\ncaptures semantic similarities between conditions and mod-\nulates tailor activation accordingly. This behavior un-\nderpins DivControl\u2019s strong zero-shot performance by en-\nabling knowledge reuse for semantically related yet unseen\nconditions without retraining. Moreover, the emergence of\nsuch alignment from condition instructions alone validates\ntheir effectiveness as condition representations and under-\nscores the semantic sensitivity of the gating mechanism.\n5.4.3. Analysis on Position and Weight of REPA\nWe analyze the impact of REPA\u2019s alignment depth and\nloss weight \u03bbon training convergence. As shown in Ta-\nble 4, applying REPA at an intermediate depth (layer 4)\nyields the fastest convergence by balancing early-stage se-\n\n--- Page 8 ---\nDEPTHCANNYHEDOUTPAINTINGINPAINTINGLINEARTPALETTELINEART\nFigure 5. Multi-Conditional Controllable Image Generation. DivControl leverages knowledge diversion to encapsulate condition-\nspecific knowledge into tailors, enabling flexible composition of multiple conditions. This facilitates high-fidelity, semantically aligned\ngeneration under simultaneous multi-condition guidance.\nCANNYHEDSKETCHLINEARTDEPTHNORMALBBOXSEGMENTATIONOUTPAINTINGINPAINTINGCANNYHEDSKETCHLINEARTDEPTHNORMALBBOXSEGMENTATIONOUTPAINTINGINPAINTING\n\u20130.20.00.20.40.60.81.0\nSKETCH\nLINEART\nFigure 6. Inter-condition similarity derived from dynamic gate ac-\ntivations ( \u03b1in Eq. (3)), illustrating the gate\u2019s capacity to capture\nsemantic relationships among conditions.\nTable 4. Impact of alignment depth and weight (i.e., \u03bbin Eq. (6))\nin REPA on convergence speed and stability.\nDepth Weight LPIPS\u2193SSIM\u2191CLIP-I \u2191MSE\u2193FID\u2193FDD\u2193\n2 0.2 0.301 0.685 90.73 40.22 10.44 0.046\n6 0.2 0.303 0.682 90.70 40.34 10.44 0.046\n4 0.005 0.307 0.681 90.61 40.47 10.73 0.046\n4 0.5 0.315 0.672 89.98 40.76 11.01 0.053\n4 0.05 0.290 0.696 91.31 39.84 9.73 0.040\nmantic guidance and late-stage task-specific adaptation. In\ncontrast, shallow alignment lacks semantic expressiveness,\nwhile deeper alignment introduces condition-specific noise\nthat impairs stability.\nAdditionally, moderate alignment strength ( \u03bb= 0.05)\nprovides effective regularization without overly constrain-ing feature learning. Excessive weighting hampers adapt-\nability, while insufficient weighting weakens alignment sig-\nnals. These results highlight the importance of carefully\nconfiguring REPA to ensure efficient and stable optimiza-\ntion.\n6. Conclusion\nIn this work, we introduce DivControl, a novel training\nframework for ControlNet that performs knowledge diver-\nsion to construct a modular architecture during pretrain-\ning. By disentangling condition-agnostic knowledge into\nreusable learngenes and encoding condition-specific knowl-\nedge into lightweight tailors, DivControl supports dynamic,\ncondition-aware assembly across diverse conditions. Div-\nControl supports zero-shot generalization to semantically\nrelated conditions via a dynamic gate, and allows efficient\nadaptation to large distribution shifts through plug-in ran-\ndomly initialized tailors. Extensive experiments demon-\nstrate that DivControl consistently outperforms existing\nmethods while substantially reducing computational over-\nhead.\nAcknowledgement\nWe sincerely appreciate Freepik for contributing to the\nfigure design. This research was supported by the\nJiangsu Science Foundation (BK20243012, BG2024036,\nBK20230832), the National Science Foundation of China\n(62125602, U24A20324, 92464301, 62306073), China\nPostdoctoral Science Foundation (2022M720028), and the\nXplorer Prize.\nReferences\n[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Ji-\naming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala,\n\n--- Page 9 ---\nTimo Aila, Samuli Laine, et al. ediff-i: Text-to-image dif-\nfusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324 , 2022. 1, 2\n[2] Junsong Chen, Simian Luo, and Enze Xie. Pixart- \u03b4: Fast and\ncontrollable image generation with latent consistency mod-\nels. In ICML 2024 Workshop on Theoretical Foundations of\nFoundation Models . 2, 3, 5\n[3] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M \u00a8uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified\nflow transformers for high-resolution image synthesis. In\nProceedings of International Conference on Machine Learn-\ning (ICML\u201924) , pages 1\u201313, 2024. 1, 3\n[4] Fu Feng, Jing Wang, Congzhi Zhang, Wenqian Li, Xu Yang,\nand Xin Geng. Genes in intelligent agents. arXiv preprint\narXiv:2306.10225 , 2023. 3\n[5] Fu Feng, Jing Wang, and Xin Geng. Transferring core\nknowledge via learngenes. arXiv preprint arXiv:2401.08139 ,\n2024. 3\n[6] Fu Feng, Yucheng Xie, Jing Wang, and Xin Geng. Wave:\nWeight template for adaptive initialization of variable-sized\nmodels. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR\u201925) , pages\n1\u201310, 2025. 3\n[7] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,\nShean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-\nrank adaptation of large language models. In Proceedings of\nthe International Conference on Learning Representations\n(ICLR\u201922) , pages 1\u201313, 2022. 2\n[8] Minghui Hu, Jianbin Zheng, Daqing Liu, Chuanxia Zheng,\nChaoyue Wang, Dacheng Tao, and Tat-Jen Cham. Cock-\ntail: Mixing multi-modality control for text-conditional im-\nage generation. In Thirty-seventh Conference on Neural In-\nformation Processing Systems , 2023. 2\n[9] Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei\nZhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang,\nand Jingdong Wang. No other representation component is\nneeded: Diffusion transformers can provide representation\nguidance by themselves. arXiv preprint arXiv:2505.02831 ,\n2025. 4\n[10] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang\nXing, Saining Xie, and Liang Zheng. Repa-e: Unlocking\nvae for end-to-end tuning with latent diffusion transformers.\narXiv preprint arXiv:2504.10483 , 2025. 4\n[11] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll \u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer vision\u2013ECCV 2014: 13th European conference,\nzurich, Switzerland, September 6-12, 2014, proceedings,\npart v 13 , pages 740\u2013755. Springer, 2014. 2, 5\n[12] Midjourney. Midjourney.com. https : / / www .\nmidjourney.com , 2022. Accessed: 2024-11-14. 1\n[13] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian\nZhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. In Proceedings of the AAAI conference on\nartificial intelligence , pages 4296\u20134304, 2024. 1, 2, 3[14] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya\nSutskever, and Mark Chen. Glide: Towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. In International Conference on Machine Learning ,\npages 16784\u201316804. PMLR, 2022. 2\n[15] OpenAI. Gpt-4: Openai language model. https://\nopenai.com/research/gpt-4 , 2023. 5\n[16] Maxime Oquab, Timoth \u00b4ee Darcet, Th \u00b4eo Moutakanni, Huy\nV o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\nTransactions on Machine Learning Research Journal , 2024.\n2, 5\n[17] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV\u201923) , pages\n4195\u20134205, 2023. 1, 2, 3\n[18] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang,\nYingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming\nXiong, Silvio Savarese, et al. Unicontrol: A unified diffu-\nsion model for controllable visual generation in the wild. In\nNeurIPS , 2023. 2, 3, 5, 6\n[19] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125 , 1\n(2):3, 2022. 1\n[20] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim\nNeumann, Rodolphe Jenatton, Andr \u00b4e Susano Pinto, Daniel\nKeysers, and Neil Houlsby. Scaling vision with sparse mix-\nture of experts. Advances in Neural Information Processing\nSystems , 34:8583\u20138595, 2021. 2, 3\n[21] Shakker Labs. Flux.1-dev-controlnet-union-pro. https:\n//huggingface.co/Shakker- Labs/FLUX.1-\ndev-ControlNet-Union-Pro , 2024. 5\n[22] Ruixiao Shi, Fu Feng, Yucheng Xie, Jing Wang, and Xin\nGeng. Fad: Frequency adaptation and diversion for cross-\ndomain few-shot learning. arXiv preprint arXiv:2505.08349 ,\n2025. 3\n[23] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue,\nand Xinchao Wang. Ominicontrol: Minimal and uni-\nversal control for diffusion transformer. arXiv preprint\narXiv:2411.15098 , 2024. 2, 5\n[24] Yuchuan Tian, Hanting Chen, Mengyu Zheng, Yuchen\nLiang, Chao Xu, and Yunhe Wang. U-repa: Aligning diffu-\nsion u-nets to vits. arXiv preprint arXiv:2503.18414 , 2025.\n4\n[25] Haoxuan Wang, Jinlong Peng, Qingdong He, Hao Yang,\nYing Jin, Jiafu Wu, Xiaobin Hu, Yanjie Pan, Zhenye Gan,\nMingmin Chi, et al. Unicombine: Unified multi-conditional\ncombination with diffusion transformer. arXiv preprint\narXiv:2503.09277 , 2025. 2\n[26] Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, and\nDamai Dai. Auxiliary-loss-free load balancing strategy for\nmixture-of-experts. arXiv preprint arXiv:2408.15664 , 2024.\n11\n[27] QiuFeng Wang, Xin Geng, ShuXia Lin, Shi-Yu Xia, Lei Qi,\nand Ning Xu. Learngene: From open-world to your learning\n\n--- Page 10 ---\ntask. In Proceedings of the AAAI Conference on Artificial\nIntelligence (AAAI\u201922) , pages 8557\u20138565, 2022. 3\n[28] Qiufeng Wang, Xu Yang, Shuxia Lin, and Xin Geng.\nLearngene: Inheriting condensed knowledge from the\nancestry model to descendant models. arXiv preprint\narXiv:2305.02279 , 2023. 3\n[29] Ge Wu, Shen Zhang, Ruijing Shi, Shanghua Gao, Zhenyuan\nChen, Lei Wang, Zhaowei Chen, Hongcheng Gao, Yao Tang,\nJian Yang, et al. Representation entanglement for genera-\ntion: Training diffusion transformers is much easier than you\nthink. arXiv preprint arXiv:2507.01467 , 2025. 4\n[30] Yucheng Xie, Fu Feng, Ruixiao Shi, Jing Wang, Yong Rui,\nand Xin Geng. Kind: Knowledge integration and diversion\nfor training decomposable models. In Forty-second Interna-\ntional Conference on Machine Learning , 2025. 2, 3\n[31] Yifeng Xu, Zhenliang He, Shiguang Shan, and Xilin Chen.\nCtrlora: An extensible and efficient framework for control-\nlable image generation. arXiv preprint arXiv:2410.09400 ,\n2024. 2, 3, 5, 6\n[32] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon\nJeong, Jonathan Huang, Jinwoo Shin, and Saining Xie.\nRepresentation alignment for generation: Training diffusion\ntransformers is easier than you think. In Proceedings of\nthe International Conference on Learning Representations\n(ICLR\u201925) , pages 1\u201317, 2024. 2, 4\n[33] Denis Zavadski, Johann-Friedrich Feiden, and Carsten\nRother. Controlnet-xs: Rethinking the control of text-to-\nimage diffusion models as feedback-control systems. In\nEuropean Conference on Computer Vision , pages 343\u2013362.\nSpringer, 2024. 1, 2, 3\n[34] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV\u201923) , pages 3836\u20133847, 2023. 1, 2,\n3\n[35] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin\nBao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K Wong.\nUni-controlnet: All-in-one control to text-to-image diffusion\nmodels. Advances in Neural Information Processing Sys-\ntems, 36:11127\u201311150, 2023. 2\n[36] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang,\nVincent Zhao, Andrew M Dai, Quoc V Le, James Laudon,\net al. Mixture-of-experts with expert choice routing. Ad-\nvances in Neural Information Processing Systems , 35:7103\u2013\n7114, 2022. 2, 3\n\n--- Page 11 ---\nA. More Details on Methods\nA.1. Dynamic Gate with Loss-Free Balancing\nTo ensure stable knowledge diversion across multiple tai-\nlors, we adopt a lightweight balancing strategy introduced\nin [26] that promotes uniform activation without relying\non auxiliary loss terms. This mechanism mitigates over-\nreliance on a subset of tailors and encourages balanced uti-\nlization, thereby enhancing specialization and condition-\nspecific diversity.\nFor a given condition with instruction etxt, the activation\nscores \u03b1= [s1, s2, ..., s NT]are computed as described in\nEq. (3). Each tailor Tiis assigned a learnable bias bi, which\nis added to its activation score siderived from the condi-\ntion embedding. The dynamic gate selects the top- Ktailors\nbased on the biased scores si+bi:\ngi=(\nsi,ifsi+bi\u2208Top-K({sj+bj}NT\nj=1)\n0,otherwise(A.1)\nThis mechanism adjusts tailor activation without affecting\noutput magnitudes or introducing gradient interference. The\nbias terms biare updated after each batch based on usage\nstatistics. Overused tailors have their biases reduced, while\nunderused ones are increased to encourage activation. This\nadaptive process maintains load balance without affecting\nthe training objective or model stability.\nA.2. More Details on Knowledge Diversion\nAlgorithm 1 presents the pseudo-code for diverting\ncondition-agnostic knowledge into reusable learngenes\nand condition-specific knowledge into lightweight tailors\nvia structured SVD-based decomposition during multi-\ncondition training.\nB. Training Details\nB.1. Dataset\nTo enable controllable generation and condition generaliza-\ntion, we construct a diverse set of conditions, as illustrated\nin Fig. B.1. Basic conditions are used for learngene-driven\nknowledge diversion, while novel conditions are introduced\nto evaluate zero-shot and few-shot generalization.\nB.2. Hyper-parameters\nTable B.1 presents the basic settings, including learning\nrate, training steps and the number of learngene compo-\nnents NGand tailor components NTfor DivControl divert-\ning knowledge.Algorithm 1 Knowledge Diversion in Controllable Image\nGeneration\nInput : ControlNet F, training dataset D={(x(i), y(i), c(i))}m\ni=1\nwithNbasicbasic conditions, number of epochs Nep, batch size B,\nlearning rate \u03b1\nOutput : Learngene G\n1:Randomly initialize the weight matrices WofF, as well as\nthe matrices U(l)\n\u22c6,\u03a3(l)\n\u22c6, andV(l)\n\u22c6, and dynamic gate G\n2:forep= 1toNepdo\n3: foreach batch {(xi, yi)}B\ni=1do\n4: Update WoffwithU(l)\n\u22c6,\u03a3(l)\n\u22c6andV(l)\n\u22c6under the rule\nof Eq. (2)\n5: Compute routing weights \u03b1i=G(eci)using Eq. (3)\n6: For each xi, forward propagate \u02c6yi=F(xi, \u03b1\u00b7\u03b8)\n7: Calculate Lbatch =1\nBPB\ni=1L(\u02c6yi, yi)according to\nEq. (1)\n8: Backward propagate the loss L(\u02c6yi, yi)to compute\nthe gradients with respect to U(l)\n\u22c6,\u03a3(l)\n\u22c6andV(l)\n\u22c6:\n\u2207ULbatch,\u2207\u03a3Lbatchand\u2207VLbatch\n9: Update the learngenes U(l)\nG,\u22c6,\u03a3(l)\nG,\u22c6andV(l)\nG,\u22c6:\nU(l)\nG,\u22c6:=U(l)\nG,\u22c6\u2212\u03b1\u00b7 \u2207ULbatch,\n\u03a3(l)\nG,\u22c6:= \u03a3(l)\nG,\u22c6\u2212\u03b1\u00b7 \u2207\u03a3Lbatch\nV(l)\nG,\u22c6:=V(l)\nG,\u22c6\u2212\u03b1\u00b7 \u2207VLbatch\n10: Update the tailors U(l)\nTi,\u22c6,\u03a3(l)\nTi,\u22c6andV(l)\nTi,\u22c6:\nU(l)\nTi,\u22c6:=U(l)\nTi,\u22c6\u2212\u03b1\u00b7G(\u2207ULbatch)\n\u03a3(l)\nTi,\u22c6:= \u03a3(l)\nTi,\u22c6\u2212\u03b1\u00b7G(\u2207\u03a3Lbatch)\nV(l)\nTi,\u22c6:=V(l)\nTi,\u22c6\u2212\u03b1\u00b7G(\u2207VLbatch)\n11: end for\n12:end for\nTable B.1. Hyper-parameters for DivControl diverting knowledge\non basic conditions.\nTraining Settings Configuration\noptimizer AdamW\nscheduler MultiStepLR\ndecay step 300K\nfactor 0.4\nlearning rate 1.25e-5\nweight decay 3e-2\nbatch size 16\ntraining steps 450,000\nimage size 512 \u00d7512\ndropout 0.1\nNG 512\nNT 512\nREPA layer 4\n\u03bb 0.05\n\n--- Page 12 ---\nILLUSION\nGRAYSCALE\nBLUR\nINPAINTING\nLINEART\nBRUSH\nBBOX\nSKETCH\nCANNY\nDEPTH\nHED\nSEGMENTATION\nNORMAL\nOUTPAINTING\nPIXEL\nJPEG\nPALETTE\nSHUFFLE\nOriginal(a)(b)(c)Figure B.1. Conditions for Training and Evaluation. (a) Eight basic conditions used for knowledge diversion and unified controllable\ngeneration. (b) Low-shift novel conditions for evaluating zero-shot generalization. (c) High-shift novel conditions for assessing few-shot\nadaptation.\nC. Additional Results\nC.1. More Results on Zero-shot Generalization\nTo further validate the zero-shot generalization ability of\nDivControl, we provide additional qualitative and quanti-\ntative results under novel control conditions not seen dur-\ning training. As shown in Fig. C.2, DivControl consistently\nmaintains semantic alignment and visual fidelity without\nany adaptation, demonstrating its strong transferability and\nrobustness under diverse unseen conditions.\nC.2. More Results on Multi-Conditional Control-\nlable Generation\nWe present additional results on multi-conditional control-\nlable generation, showcasing DivControl\u2019s ability to inte-\ngrate multiple control conditions. We evaluate various con-\ndition combinations, including L INEART , PALETTE , and\nOUTPAINTING , demonstrating DivControl\u2019s seamless syn-\nthesis of these conditions into coherent outputs. As shown\nin Fig. C.1, DivControl consistently generates high-quality,\nsemantically aligned images, underscoring its flexibility\nand efficient feature reuse.\n\n--- Page 13 ---\nDEPTHCANNYHEDOUTPAINTINGINPAINTINGLINEARTPALETTELINEART\nFigure C.1. More Results on Multi-Conditional Controllable Image Generation.\n\n--- Page 14 ---\nConditionOriginalUniControlCtrLoRADivControlConditionOriginalUniControlCtrLoRADivControl\nILLUSION\nILLUSION\nILLUSION\nILLUSION\nGRAYSCALE\nGRAYSCALE\nGRAYSCALE\nGRAYSCALE\nBLUR\nBLUR\nBLUR\nBLUR\nINPAINTING\nINPAINTING\nINPAINTING\nINPAINTING\nLINEART\nLINEART\nLINEART\nLINEART\nBRUSH\nBRUSH\nBRUSH\nBRUSHFigure C.2. More Results on Zero-shot Generalization on Low-shift Novel Conditions.",
  "project_dir": "artifacts/projects/enhanced_cs.LG_2507.23620v1_DivControl_Knowledge_Diversion_for_Controllable_I",
  "communication_dir": "artifacts/projects/enhanced_cs.LG_2507.23620v1_DivControl_Knowledge_Diversion_for_Controllable_I/.agent_comm",
  "assigned_at": "2025-08-03T20:59:43.864403",
  "status": "assigned"
}